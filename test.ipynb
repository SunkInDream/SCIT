{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_impute import *\n",
    "from pygrinder import block_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/lorenz/lorenz_dataset_0_timeseries.csv\",header=None)\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727148",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = pd.read_csv('./causality_matrices/lorenz_causality_matrix.csv', header=None)\n",
    "cg = cg.values\n",
    "model_params = {\n",
    "        'num_levels': 10,\n",
    "        'kernel_size': 8,\n",
    "        'dilation_c': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = block_missing(data[np.newaxis,...],factor=0.1, block_len=3, block_width=3)\n",
    "data = data[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3939b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = impute(data, cg, model_params)\n",
    "data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_miracle.py\n",
    "import numpy as np\n",
    "from baseline import miracle_impu\n",
    "\n",
    "# 创建测试数据\n",
    "test_data = np.random.randn(100, 10)\n",
    "test_data[np.random.random((100, 10)) < 0.1] = np.nan\n",
    "\n",
    "print(\"测试数据形状:\", test_data.shape)\n",
    "print(\"缺失值数量:\", np.isnan(test_data).sum())\n",
    "\n",
    "try:\n",
    "    result = miracle_impu(test_data)\n",
    "    print(\"MIRACLE结果形状:\", result.shape if result is not None else \"None\")\n",
    "    print(\"MIRACLE结果类型:\", type(result))\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"填补后缺失值:\", np.isnan(result).sum())\n",
    "    else:\n",
    "        print(\"❌ MIRACLE返回了None\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ MIRACLE测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf579a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取 CSV 文件\n",
    "zero_matrix = pd.read_csv(\"zero_impu_matrix.csv\").values  # shape: (T, N)\n",
    "\n",
    "# 构造掩码矩阵：0 的位置为 0，其余为 1\n",
    "mask_matrix = np.where(zero_matrix == 0, 0, 1)\n",
    "\n",
    "# 保存新矩阵到 CSV\n",
    "pd.DataFrame(mask_matrix).to_csv(\"zero_impu_mask.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10119",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv('./train_result.csv').values\n",
    "gt = pd.read_csv('./gt_matrix.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbe677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = 1 - mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64647142",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = F.mse_loss(torch.tensor(pred*mask_matrix), torch.tensor(gt*mask_matrix))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "tmp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "tmp_file.close()\n",
    "os.remove(tmp_file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a02438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 原始目录和目标目录路径\n",
    "src_dir = './data/III'        # 替换为你的源目录路径\n",
    "dst_dir = './data/mimic-iii'   # 替换为你的目标目录路径\n",
    "\n",
    "# 创建目标目录（如果不存在）\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# 获取源目录中的所有文件名（按名称排序，可改为按修改时间等）\n",
    "all_files = sorted(f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f)))\n",
    "\n",
    "# 选择前100个文件\n",
    "files_to_move = all_files[:100]\n",
    "\n",
    "# 移动文件\n",
    "for fname in files_to_move:\n",
    "    shutil.move(os.path.join(src_dir, fname), os.path.join(dst_dir, fname))\n",
    "\n",
    "print(f\"已成功移动 {len(files_to_move)} 个文件到 {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from baseline import *\n",
    "from models_impute import *\n",
    "# 指定目录\n",
    "data_dir = \"./data/III\"  # 替换为你的目录路径\n",
    "\n",
    "# 获取目录下第一个 CSV 文件\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "assert csv_files, \"目录中没有找到 CSV 文件\"\n",
    "csv_path = os.path.join(data_dir, csv_files[0])\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(csv_path)\n",
    "mx = df.values.astype(np.float32)\n",
    "\n",
    "# 记录原始缺失位置\n",
    "nan_pos = np.argwhere(np.isnan(mx))\n",
    "\n",
    "# 填补缺失值\n",
    "imputed_mx = timesnet_impu(mx,)\n",
    "\n",
    "# 打印填补前后的值（仅缺失位置）\n",
    "print(f\"填补前后对比（仅缺失位置）：{imputed_mx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import sys\n",
    "\n",
    "for _, name, _ in pkgutil.iter_modules():\n",
    "    if 'tsde' in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygrinder import mcar, mar_logistic, mnar_x\n",
    "\n",
    "def dig_missing_values(\n",
    "    file_path: str,\n",
    "    output_dir: str,\n",
    "    obs_rate: float = 0.6,\n",
    "    mar_missing_rate: float = 0.4,\n",
    "    mnar_offset: float = 0.7,\n",
    "    mcar_p: float = 0.1\n",
    "):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = df.values.astype(np.float32)\n",
    "    \n",
    "    # 应用缺失机制\n",
    "    X = mar_logistic(data, obs_rate=obs_rate, missing_rate=mar_missing_rate)\n",
    "    X = X[np.newaxis, ...]                  # 添加 batch 维度\n",
    "    X = mnar_x(X, offset=mnar_offset)       # MNAR 缺失\n",
    "    X = mcar(X, p=mcar_p)                   # 再次添加 MCAR 缺失\n",
    "    X = X.squeeze(0)                        # 去除 batch 维度\n",
    "\n",
    "    # 保存结果\n",
    "    filename = os.path.basename(file_path).replace('.csv', '_dig_missing.csv')\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    pd.DataFrame(X).to_csv(output_path, index=False)\n",
    "    print(f\"✅ 保存至: {output_path}\")\n",
    "dig_missing_values(\n",
    "    file_path='./data/air/2013-03-07.csv',\n",
    "    output_dir='./',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdb\n",
    "\n",
    "# list all available datasets in TSDB\n",
    "tsdb.list()\n",
    "# ['physionet_2012',\n",
    "#  'physionet_2019',\n",
    "#  'electricity_load_diagrams',\n",
    "#  'beijing_multisite_air_quality',\n",
    "#  'italy_air_quality',\n",
    "#  'vessel_ais',\n",
    "#  'electricity_transformer_temperature',\n",
    "#  'pems_traffic',\n",
    "#  'solar_alabama',\n",
    "#  'ucr_uea_ACSF1',\n",
    "#  'ucr_uea_Adiac',\n",
    "#  ...\n",
    "\n",
    "tsdb.download_and_extract('beijing_multisite_air_quality', './save_it_here')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 用户自定义路径 ====\n",
    "input_file = './save_it_here/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv'       # 输入文件路径\n",
    "output_dir = './data/air'     # 输出文件夹路径\n",
    "\n",
    "# ==== 需要提取的列 ====\n",
    "columns_needed = [\"year\", \"month\", \"day\", \"hour\", \"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\",\n",
    "                  \"O3\", \"TEMP\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\"]\n",
    "\n",
    "# ==== 创建输出目录（如果不存在） ====\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==== 读取数据并提取指定列 ====\n",
    "df = pd.read_csv(input_file, usecols=columns_needed)\n",
    "\n",
    "# ==== 按日期分组（year, month, day） ====\n",
    "grouped = df.groupby(['year', 'month', 'day'])\n",
    "\n",
    "# ==== 遍历每一天的数据 ====\n",
    "for (y, m, d), group in grouped:\n",
    "    # 检查是否为整天（24小时）且无缺失值\n",
    "    if len(group) == 24 and not group.isnull().values.any():\n",
    "        # 构造保存文件名\n",
    "        filename = f\"{y:04d}-{m:02d}-{d:02d}.csv\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        group.to_csv(save_path, index=False,)\n",
    "        print(f\"✅ 已保存: {filename}\")\n",
    "\n",
    "print(\"🎉 所有完整天数据提取完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e56633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_last_n_files(folder_path, n, sort_by='name'):\n",
    "    \"\"\"\n",
    "    删除指定文件夹下的最后 n 个文件。\n",
    "    \n",
    "    :param folder_path: 文件夹路径\n",
    "    :param n: 要删除的文件个数\n",
    "    :param sort_by: 'name' 或 'mtime'，按文件名或最后修改时间排序\n",
    "    \"\"\"\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "             if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if sort_by == 'name':\n",
    "        files.sort()  # 按文件名排序\n",
    "    elif sort_by == 'mtime':\n",
    "        files.sort(key=os.path.getmtime)  # 按修改时间排序\n",
    "    else:\n",
    "        raise ValueError(\"sort_by must be 'name' or 'mtime'\")\n",
    "\n",
    "    to_delete = files[-n:]  # 取最后 n 个文件\n",
    "\n",
    "    for file_path in to_delete:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"✅ 已删除: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 删除失败: {file_path}, 错误: {e}\")\n",
    "\n",
    "# 示例用法\n",
    "delete_last_n_files(\"./data/air\", n=575, sort_by='name')  # 或 sort_by='mtime'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_matching_files(dir_a, dir_b, dir_c):\n",
    "    if not os.path.exists(dir_c):\n",
    "        os.makedirs(dir_c)\n",
    "\n",
    "    # 获取目录 B 中所有文件名（不含路径）\n",
    "    b_filenames = set(os.listdir(dir_b))\n",
    "\n",
    "    # 遍历目录 A，查找与目录 B 同名的文件\n",
    "    for filename in os.listdir(dir_a):\n",
    "        if filename in b_filenames:\n",
    "            src_path = os.path.join(dir_a, filename)\n",
    "            dst_path = os.path.join(dir_c, filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(f\"✅ 已复制: {filename}\")\n",
    "\n",
    "# ✅ 示例使用：\n",
    "dir_a = \"./data/III\"# 替换为实际路径\n",
    "dir_b = \"./data_imputed/my_model/III\"\n",
    "dir_c = \"./data/downstreamIII\"  # 替换为实际路径\n",
    "copy_matching_files(dir_a, dir_b, dir_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e70264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pad_csv_files_to_193_rows(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue  # 跳过非 CSV 文件\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            original_rows = len(df)\n",
    "            target_rows = 192  # 不包含表头的部分\n",
    "\n",
    "            if original_rows < target_rows:\n",
    "                # 获取最后一行（如果为空则跳过）\n",
    "                if original_rows == 0:\n",
    "                    print(f\"⚠️ 文件 {filename} 是空的，跳过\")\n",
    "                    continue\n",
    "\n",
    "                last_row = df.iloc[[-1]]  # 保持 DataFrame 格式\n",
    "                rows_to_add = target_rows - original_rows\n",
    "\n",
    "                # 重复追加最后一行\n",
    "                df = pd.concat([df] + [last_row] * rows_to_add, ignore_index=True)\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"✅ 文件 {filename} 补齐至 {target_rows+1} 行（含表头）\")\n",
    "\n",
    "            elif original_rows > target_rows:\n",
    "                print(f\"✅ 文件 {filename} 已满足行数要求 ({original_rows+1} 行含表头)\")\n",
    "\n",
    "            else:\n",
    "                print(f\"✅ 文件 {filename} 恰好 {target_rows+1} 行（含表头）\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理文件 {filename} 出错: {e}\")\n",
    "\n",
    "# ✅ 示例用法\n",
    "target_dir = \"./data_imputed/my_model/III\"  # 替换为实际目录路径\n",
    "pad_csv_files_to_193_rows(target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb2576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 启动 4 个 GPU 并行填补，共 1367 个文件\n",
      "✅ 所有文件处理完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/data/anaconda3/envs/Former/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# saits_parallel.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from multiprocessing import Process, Queue, current_process\n",
    "from tqdm import tqdm\n",
    "\n",
    "def saits_impu(mx, epochs=50, d_model=32, n_layers=2, n_heads=4,\n",
    "               d_k=8, d_v=8, d_ffn=16, dropout=0.4, device=None):\n",
    "    from pypots.imputation import SAITS\n",
    "\n",
    "    global_mean = np.nanmean(mx)\n",
    "    all_nan_cols = np.all(np.isnan(mx), axis=0)\n",
    "    if all_nan_cols.any():\n",
    "        print(f\"发现 {all_nan_cols.sum()} 列全为NaN，这些列将用填充\")\n",
    "        mx[:, all_nan_cols] = global_mean\n",
    "\n",
    "    mx = mx.copy()\n",
    "    n_steps, n_features = mx.shape\n",
    "    data_3d = mx[np.newaxis, :, :]\n",
    "\n",
    "    saits = SAITS(\n",
    "        n_steps=n_steps,\n",
    "        n_features=n_features,\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        d_k=d_k,\n",
    "        d_v=d_v,\n",
    "        d_ffn=d_ffn,\n",
    "        dropout=dropout,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    train_set = {\"X\": data_3d}\n",
    "    saits.fit(train_set)\n",
    "    imputed = saits.impute({\"X\": data_3d})\n",
    "    return imputed[0]\n",
    "\n",
    "def gpu_worker(file_queue, input_dir, output_dir, gpu_id):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pid = current_process().pid\n",
    "\n",
    "    while not file_queue.empty():\n",
    "        try:\n",
    "            fname = file_queue.get_nowait()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            fpath = os.path.join(input_dir, fname)\n",
    "            mx = pd.read_csv(fpath).values.astype(np.float32)\n",
    "            filled = saits_impu(mx, device=device)\n",
    "\n",
    "            out_path = os.path.join(output_dir, fname)\n",
    "            pd.DataFrame(filled).to_csv(out_path, index=False)\n",
    "            print(f\"[PID {pid} GPU {gpu_id}] ✅ 处理完成: {fname}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[PID {pid} GPU {gpu_id}] ❌ 处理失败: {fname}，错误：{e}\")\n",
    "\n",
    "def parallel_saits_impute(input_dir, output_dir):\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_list = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    file_queue = mp.Queue()\n",
    "    for fname in file_list:\n",
    "        file_queue.put(fname)\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    processes = []\n",
    "\n",
    "    print(f\"🚀 启动 {num_gpus} 个 GPU 并行填补，共 {len(file_list)} 个文件\")\n",
    "\n",
    "    for gpu_id in range(num_gpus):\n",
    "        p = mp.Process(target=gpu_worker, args=(file_queue, input_dir, output_dir, gpu_id))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    print(\"✅ 所有文件处理完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import multiprocessing as mp\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    input_dir = \"./data/downstreamIII\"\n",
    "    output_dir = \"./data_imputed/saits/III\"\n",
    "    parallel_saits_impute(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf869f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "tar = pd.read_csv('./data/mimic/200068.csv').values\n",
    "print(tar)\n",
    "from baseline import *\n",
    "res = knn_impu(tar, k=5)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
