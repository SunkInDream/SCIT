{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_impute import *\n",
    "from pygrinder import block_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/lorenz/lorenz_dataset_0_timeseries.csv\",header=None)\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727148",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = pd.read_csv('./causality_matrices/lorenz_causality_matrix.csv', header=None)\n",
    "cg = cg.values\n",
    "model_params = {\n",
    "        'num_levels': 10,\n",
    "        'kernel_size': 8,\n",
    "        'dilation_c': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = block_missing(data[np.newaxis,...],factor=0.1, block_len=3, block_width=3)\n",
    "data = data[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3939b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = impute(data, cg, model_params)\n",
    "data_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_miracle.py\n",
    "import numpy as np\n",
    "from baseline import miracle_impu\n",
    "\n",
    "# 创建测试数据\n",
    "test_data = np.random.randn(100, 10)\n",
    "test_data[np.random.random((100, 10)) < 0.1] = np.nan\n",
    "\n",
    "print(\"测试数据形状:\", test_data.shape)\n",
    "print(\"缺失值数量:\", np.isnan(test_data).sum())\n",
    "\n",
    "try:\n",
    "    result = miracle_impu(test_data)\n",
    "    print(\"MIRACLE结果形状:\", result.shape if result is not None else \"None\")\n",
    "    print(\"MIRACLE结果类型:\", type(result))\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"填补后缺失值:\", np.isnan(result).sum())\n",
    "    else:\n",
    "        print(\"❌ MIRACLE返回了None\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ MIRACLE测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf579a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取 CSV 文件\n",
    "zero_matrix = pd.read_csv(\"zero_impu_matrix.csv\").values  # shape: (T, N)\n",
    "\n",
    "# 构造掩码矩阵：0 的位置为 0，其余为 1\n",
    "mask_matrix = np.where(zero_matrix == 0, 0, 1)\n",
    "\n",
    "# 保存新矩阵到 CSV\n",
    "pd.DataFrame(mask_matrix).to_csv(\"zero_impu_mask.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10119",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv('./train_result.csv').values\n",
    "gt = pd.read_csv('./gt_matrix.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbe677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = 1 - mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64647142",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt*mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = F.mse_loss(torch.tensor(pred*mask_matrix), torch.tensor(gt*mask_matrix))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "tmp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "tmp_file.close()\n",
    "os.remove(tmp_file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a02438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 原始目录和目标目录路径\n",
    "src_dir = './data/III'        # 替换为你的源目录路径\n",
    "dst_dir = './data/mimic-iii'   # 替换为你的目标目录路径\n",
    "\n",
    "# 创建目标目录（如果不存在）\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# 获取源目录中的所有文件名（按名称排序，可改为按修改时间等）\n",
    "all_files = sorted(f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f)))\n",
    "\n",
    "# 选择前100个文件\n",
    "files_to_move = all_files[:100]\n",
    "\n",
    "# 移动文件\n",
    "for fname in files_to_move:\n",
    "    shutil.move(os.path.join(src_dir, fname), os.path.join(dst_dir, fname))\n",
    "\n",
    "print(f\"已成功移动 {len(files_to_move)} 个文件到 {dst_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from baseline import *\n",
    "from models_impute import *\n",
    "# 指定目录\n",
    "data_dir = \"./data/III\"  # 替换为你的目录路径\n",
    "\n",
    "# 获取目录下第一个 CSV 文件\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "assert csv_files, \"目录中没有找到 CSV 文件\"\n",
    "csv_path = os.path.join(data_dir, csv_files[0])\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(csv_path)\n",
    "mx = df.values.astype(np.float32)\n",
    "\n",
    "# 记录原始缺失位置\n",
    "nan_pos = np.argwhere(np.isnan(mx))\n",
    "\n",
    "# 填补缺失值\n",
    "imputed_mx = timesnet_impu(mx,)\n",
    "\n",
    "# 打印填补前后的值（仅缺失位置）\n",
    "print(f\"填补前后对比（仅缺失位置）：{imputed_mx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import sys\n",
    "\n",
    "for _, name, _ in pkgutil.iter_modules():\n",
    "    if 'tsde' in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pygrinder import mcar, mar_logistic, mnar_x\n",
    "\n",
    "def dig_missing_values(\n",
    "    file_path: str,\n",
    "    output_dir: str,\n",
    "    obs_rate: float = 0.6,\n",
    "    mar_missing_rate: float = 0.4,\n",
    "    mnar_offset: float = 0.7,\n",
    "    mcar_p: float = 0.1\n",
    "):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = df.values.astype(np.float32)\n",
    "    \n",
    "    # 应用缺失机制\n",
    "    X = mar_logistic(data, obs_rate=obs_rate, missing_rate=mar_missing_rate)\n",
    "    X = X[np.newaxis, ...]                  # 添加 batch 维度\n",
    "    X = mnar_x(X, offset=mnar_offset)       # MNAR 缺失\n",
    "    X = mcar(X, p=mcar_p)                   # 再次添加 MCAR 缺失\n",
    "    X = X.squeeze(0)                        # 去除 batch 维度\n",
    "\n",
    "    # 保存结果\n",
    "    filename = os.path.basename(file_path).replace('.csv', '_dig_missing.csv')\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    pd.DataFrame(X).to_csv(output_path, index=False)\n",
    "    print(f\"✅ 保存至: {output_path}\")\n",
    "dig_missing_values(\n",
    "    file_path='./data/air/2013-03-07.csv',\n",
    "    output_dir='./',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdb\n",
    "\n",
    "# list all available datasets in TSDB\n",
    "tsdb.list()\n",
    "# ['physionet_2012',\n",
    "#  'physionet_2019',\n",
    "#  'electricity_load_diagrams',\n",
    "#  'beijing_multisite_air_quality',\n",
    "#  'italy_air_quality',\n",
    "#  'vessel_ais',\n",
    "#  'electricity_transformer_temperature',\n",
    "#  'pems_traffic',\n",
    "#  'solar_alabama',\n",
    "#  'ucr_uea_ACSF1',\n",
    "#  'ucr_uea_Adiac',\n",
    "#  ...\n",
    "\n",
    "tsdb.download_and_extract('beijing_multisite_air_quality', './save_it_here')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== 用户自定义路径 ====\n",
    "input_file = './save_it_here/PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv'       # 输入文件路径\n",
    "output_dir = './data/air'     # 输出文件夹路径\n",
    "\n",
    "# ==== 需要提取的列 ====\n",
    "columns_needed = [\"year\", \"month\", \"day\", \"hour\", \"PM2.5\", \"PM10\", \"SO2\", \"NO2\", \"CO\",\n",
    "                  \"O3\", \"TEMP\", \"PRES\", \"DEWP\", \"RAIN\", \"WSPM\"]\n",
    "\n",
    "# ==== 创建输出目录（如果不存在） ====\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==== 读取数据并提取指定列 ====\n",
    "df = pd.read_csv(input_file, usecols=columns_needed)\n",
    "\n",
    "# ==== 按日期分组（year, month, day） ====\n",
    "grouped = df.groupby(['year', 'month', 'day'])\n",
    "\n",
    "# ==== 遍历每一天的数据 ====\n",
    "for (y, m, d), group in grouped:\n",
    "    # 检查是否为整天（24小时）且无缺失值\n",
    "    if len(group) == 24 and not group.isnull().values.any():\n",
    "        # 构造保存文件名\n",
    "        filename = f\"{y:04d}-{m:02d}-{d:02d}.csv\"\n",
    "        save_path = os.path.join(output_dir, filename)\n",
    "        group.to_csv(save_path, index=False,)\n",
    "        print(f\"✅ 已保存: {filename}\")\n",
    "\n",
    "print(\"🎉 所有完整天数据提取完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e56633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_last_n_files(folder_path, n, sort_by='name'):\n",
    "    \"\"\"\n",
    "    删除指定文件夹下的最后 n 个文件。\n",
    "    \n",
    "    :param folder_path: 文件夹路径\n",
    "    :param n: 要删除的文件个数\n",
    "    :param sort_by: 'name' 或 'mtime'，按文件名或最后修改时间排序\n",
    "    \"\"\"\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "             if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if sort_by == 'name':\n",
    "        files.sort()  # 按文件名排序\n",
    "    elif sort_by == 'mtime':\n",
    "        files.sort(key=os.path.getmtime)  # 按修改时间排序\n",
    "    else:\n",
    "        raise ValueError(\"sort_by must be 'name' or 'mtime'\")\n",
    "\n",
    "    to_delete = files[-n:]  # 取最后 n 个文件\n",
    "\n",
    "    for file_path in to_delete:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"✅ 已删除: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 删除失败: {file_path}, 错误: {e}\")\n",
    "\n",
    "# 示例用法\n",
    "delete_last_n_files(\"./data/air\", n=575, sort_by='name')  # 或 sort_by='mtime'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_matching_files(dir_a, dir_b, dir_c):\n",
    "    if not os.path.exists(dir_c):\n",
    "        os.makedirs(dir_c)\n",
    "\n",
    "    # 获取目录 B 中所有文件名（不含路径）\n",
    "    b_filenames = set(os.listdir(dir_b))\n",
    "\n",
    "    # 遍历目录 A，查找与目录 B 同名的文件\n",
    "    for filename in os.listdir(dir_a):\n",
    "        if filename in b_filenames:\n",
    "            src_path = os.path.join(dir_a, filename)\n",
    "            dst_path = os.path.join(dir_c, filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(f\"✅ 已复制: {filename}\")\n",
    "\n",
    "# ✅ 示例使用：\n",
    "dir_a = \"./data/III\"# 替换为实际路径\n",
    "dir_b = \"./data_imputed/my_model/III\"\n",
    "dir_c = \"./data/downstreamIII\"  # 替换为实际路径\n",
    "copy_matching_files(dir_a, dir_b, dir_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e70264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def pad_csv_files_to_193_rows(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue  # 跳过非 CSV 文件\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            original_rows = len(df)\n",
    "            target_rows = 192  # 不包含表头的部分\n",
    "\n",
    "            if original_rows < target_rows:\n",
    "                # 获取最后一行（如果为空则跳过）\n",
    "                if original_rows == 0:\n",
    "                    print(f\"⚠️ 文件 {filename} 是空的，跳过\")\n",
    "                    continue\n",
    "\n",
    "                last_row = df.iloc[[-1]]  # 保持 DataFrame 格式\n",
    "                rows_to_add = target_rows - original_rows\n",
    "\n",
    "                # 重复追加最后一行\n",
    "                df = pd.concat([df] + [last_row] * rows_to_add, ignore_index=True)\n",
    "                df.to_csv(filepath, index=False)\n",
    "                print(f\"✅ 文件 {filename} 补齐至 {target_rows+1} 行（含表头）\")\n",
    "\n",
    "            elif original_rows > target_rows:\n",
    "                print(f\"✅ 文件 {filename} 已满足行数要求 ({original_rows+1} 行含表头)\")\n",
    "\n",
    "            else:\n",
    "                print(f\"✅ 文件 {filename} 恰好 {target_rows+1} 行（含表头）\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理文件 {filename} 出错: {e}\")\n",
    "\n",
    "# ✅ 示例用法\n",
    "target_dir = \"./data_imputed/my_model/III\"  # 替换为实际目录路径\n",
    "pad_csv_files_to_193_rows(target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb2576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE填补中:   2%|▏         | 20/1022 [01:21<1:02:39,  3.75s/it]/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:355: RuntimeWarning: overflow encountered in multiply\n",
      "  gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))\n",
      "/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:355: RuntimeWarning: invalid value encountered in divide\n",
      "  gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))\n",
      "MICE填补中:   2%|▏         | 20/1022 [01:21<1:08:18,  4.09s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nBayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, fname)\n\u001b[1;32m     42\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 43\u001b[0m filled \u001b[38;5;241m=\u001b[39m \u001b[43mmice_impu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(filled)\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, fname), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[50], line 31\u001b[0m, in \u001b[0;36mmice_impu\u001b[0;34m(mx, max_iter)\u001b[0m\n\u001b[1;32m     29\u001b[0m         model \u001b[38;5;241m=\u001b[39m BayesianRidge()\n\u001b[1;32m     30\u001b[0m         model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 31\u001b[0m         matrix_filled[missing_idx, col] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matrix_filled\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:415\u001b[0m, in \u001b[0;36mBayesianRidge.predict\u001b[0;34m(self, X, return_std)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    394\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    In addition to the mean of the predictive distribution, also its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m        Standard deviation of predictive distribution of query points.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m     y_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_std:\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y_mean\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/linear_model/_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/Former/lib/python3.8/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nBayesianRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# ==== 填补函数 ====\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import numpy as np\n",
    "\n",
    "def mice_impu(mx, max_iter=5, n_jobs=-1):\n",
    "    mx = mx.copy()\n",
    "    n_rows, n_cols = mx.shape\n",
    "    all_nan_cols = np.all(np.isnan(mx), axis=0)\n",
    "    if all_nan_cols.any():\n",
    "        global_mean = np.nanmean(mx)\n",
    "        mx[:, all_nan_cols] = global_mean\n",
    "\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    matrix_filled = imp.fit_transform(mx)\n",
    "\n",
    "    def update_column(col):\n",
    "        missing_idx = np.where(np.isnan(mx[:, col]))[0]\n",
    "        if len(missing_idx) == 0:\n",
    "            return None  # 不更新\n",
    "\n",
    "        observed_idx = np.where(~np.isnan(mx[:, col]))[0]\n",
    "        X_train = np.delete(matrix_filled[observed_idx], col, axis=1)\n",
    "        y_train = mx[observed_idx, col]\n",
    "        X_pred = np.delete(matrix_filled[missing_idx], col, axis=1)\n",
    "\n",
    "        model = BayesianRidge()\n",
    "        model.fit(X_train, y_train)\n",
    "        return col, missing_idx, model.predict(X_pred)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(update_column)(col) for col in range(n_cols))\n",
    "        for res in results:\n",
    "            if res is not None:\n",
    "                col, missing_idx, preds = res\n",
    "                matrix_filled[missing_idx, col] = preds\n",
    "\n",
    "    return matrix_filled\n",
    "\n",
    "# ==== 主逻辑 ====\n",
    "input_dir = './data/downstreamIII'\n",
    "output_dir = './data_imputed/mice/III'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for fname in tqdm(os.listdir(input_dir), desc=\"MICE填补中\"):\n",
    "    if fname.endswith('.csv'):\n",
    "        path = os.path.join(input_dir, fname)\n",
    "        data = pd.read_csv(path).values.astype(np.float32)\n",
    "        filled = mice_impu(data)\n",
    "        pd.DataFrame(filled).to_csv(os.path.join(output_dir, fname), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
